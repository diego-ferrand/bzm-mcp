# Reporting

## Report Statuses

After you run your tests, you can view and analyze the test execution results. BlazeMeter lets you know if errors have occurred so that you are aware of problems and can fix them or run the tests again.

**Use when**: Understanding test execution statuses, viewing test results, or identifying test execution errors.

### Test Execution Statuses

You can view test execution statuses from the **History**, **Recent Test Runs**, and **Reports** pages.

BlazeMeter supports the following test execution statuses:

- **Passed** - A test is deemed to pass if none of the Failure Criteria defined for the test are met
- **Failed** - A test is deemed to fail if one or more of the Failure Criteria defined for the test are met
- **Not Set** - A test that has no Failure Criteria defined
- **Aborted** - A test that is terminated using the **Abort Test** command available during the booting phase
- **Error** - A test with one or more execution errors that causes the test to end with no data. For example:
  - An engine fails to boot
  - A related service (test data, virtual service) is unavailable or returns an error
  - JMeter plugins are not installed correctly
  - There is a mistake in the test script that makes it impossible to execute such as missing files or files in an incorrect format
  - Tests that finish with no data are not included in trend charts. In some cases, following an execution error, the test ends with partial data. In that case, the test status is not Error. Instead, a warning is displayed on the report
- **No Data** - deprecated. Legacy reports with execution errors that ended with no data will remain in No Data status

### Viewing Test Statuses

In **Recent Test Runs**, you can filter test executions by status.

When a test is running, you see a blinking green dot and a *Running* notification.

On the **Reports** > **Recent Test Runs** dropdown list, you can filter by status.

On the **Reports** > **Show all reports (quick view)**, status indicators are displayed.

On the timeline, status indicators show the current state of test execution.

### Execution Errors and Warnings

If an execution error occurs, a notification is displayed in **Reports** on the **Summary** tab. If a test ends with no data, an error appears in red.

If a test ends with partial data, a warning appears in yellow.

In **Trend Charts**, you can hover over a data point to view execution errors and warnings.

### Documentation References

For detailed information about report statuses, use the BlazeMeter MCP help tools:

**Report Statuses**:
- **Category**: `root_category`
- **Subcategory**: `guide`
- **Help ID**: `performance-report-statuses`
- **Read help**: Use `blazemeter_help` with action `read_help_info`, args: `{"category_id": "root_category", "subcategory_id": "guide", "help_id_list": ["performance-report-statuses"]}`

---

## Filter Report Data

Filter performance test report data by date/time, location, scenario, and transaction status (passed/failed) to focus analysis on specific subsets of test results. You can apply filters to report data to enhance the usability and effectiveness of performance testing data analysis. By narrowing down the scope of data presented in the reports, the filtering capabilities address various business needs and use cases.

**Use when**: Filtering performance test report data by date/time, location, scenario, or transaction status or focusing analysis on specific subsets of test results.

### Filter Options

The report header, visible from all BlazeMeter reports, contains the following filters:

- **DateTime**: Filter by time range
- **Scenarios**: Filter by test scenario
- **Locations**: Filter by geographic location

### Filter by Date and Time

The **DateTime** filter lets you define a specific date and time range for your performance test reports. You can filter out data outside a specified time frame, providing a more focused view of the test results. You can also trim the ramp-up time at the beginning and end of a test. The timeline shows events that occurred during the test run, for example, the number of users was changed.

**Steps:**

1. In a report, click the timeline bar. The DateTime picker displays
2. Set the **From** and **To** dates and times
3. Click **Apply**

To restore the original timeline of the test, click **Reset**.

### Filter by Location

**Locations** lets you filter the information on the report, so only users and requests made from specific locations are listed. The **Locations** filter is disabled when there is only one location in the test run. The single location is selected and displayed by default.

**Steps:**

1. In a report, click the **Locations** field
2. Select one or more locations

### Filter by Scenario

**Scenarios** lets you filter the information on the report, so that only users and requests generated by the requested scenario are listed. The **Scenarios** filter is disabled when there is only one scenario in the test run. The single scenario is selected and displayed by default.

**Steps:**

1. In a report, click the **Scenarios** field
2. Select one or more scenarios

### Filter by Transactions

Failed transactions can increase the response time in case of session timeout error, or decrease the response time in case of an error returned immediately. For more accurate response time information, you can filter out failed transactions in the report to see results only for the transactions that passed.

BlazeMeter allows you to filter out transactions with HTTP error code and transactions with JMeter Assertion errors.

If you have failure criteria defined, you can see what is the status of each failure criteria, without the effect of the errors (however, overall Test status will not change). The filter can be applied also in the Compare Report screen and Trend Charts screen.

When selection is not possible, such as for the test results email, the default (failed and passed transactions) is shown.

The filter can apply only on test runs executed after the feature was released and the Exclude Failed Transaction flag was opened for the account.

#### Exclude Failed Transactions from a Test Report

In the report, select whether you want the test report data to include the failed transactions or not. By default, results of all transactions (failed and passed) are shown. You are filtering on the report level so the filter affects all relevant report data and tabs (Summary, Timeline Report, Request Stats and Failure Criteria).

**Steps:**

1. In the **Performance** tab, select **Reports**. The most recent reports are shown on top. You can also enter a report name in the search field to view top 5 recently executed reports
2. Click **Show all reports** and select a report to view its details. A test report overview opens. At the top of the page, you can filter by **Scenario**, **Location** and **Transactions**
3. Expand the **Transactions** drop-down field. You can choose any combination of the following transactions:
   - **Passed**
   - **Failed based on error code**
   - **Failed based on assertions**
   By default, report data for all transactions is displayed (data selected in the filter)
4. Select **Passed** transactions

#### Exclude Failed Transactions from the Executive Summary

**Steps:**

1. In the **Performance** tab, select **Reports**. The most recent reports are shown on top. You can also enter a report name in the search field to view top 5 recently executed reports
2. Click **Show all reports** and select a report to view its details. A test report overview opens
3. Click the menu next to the test name
4. From the drop-down list, select **Executive Summary**

#### Exclude Failed Transactions in Compare Report

**Steps:**

1. In the **Performance** tab, select **Reports**. The most recent reports are shown on top. You can also enter a report name in the search field to view top 5 recently executed reports
2. Click **Show all reports** and select a report to view its details. A test report overview opens
3. Click the menu next to the test name
4. From the drop-down list, select **Compare Report**
5. At the top of the page, you can filter by **Scenario**, **Location** and **Transactions**
6. Expand the **Transactions** drop-down field. You can choose any combination of the following transactions:
   - **Passed**
   - **Failed based on error code**
   - **Failed based on assertions**
   By default, report data for all transactions is displayed (data selected in the filter)
7. Select **Passed** transactions

Failed transactions will not be included in your report.

#### Exclude Failed Transactions in Trend Charts

**Steps:**

1. In the **Performance** tab, select **Tests**. The most recent reports are shown on top. You can also enter a test name in the search field to view top 5 recently updated tests
2. Click **Show all tests** and select a test to view its details. A test overview opens
3. Click the **Trend Charts** tab
4. In the **Filter by Transaction** dropdown, select **Passed**

Only passed transactions will be included in your report.

### Documentation References

For detailed information about excluding failed transactions, use the BlazeMeter MCP help tools:

**Exclude Failed Transactions**:
- **Category**: `root_category`
- **Subcategory**: `guide`
- **Help ID**: `performance-exclude-failed-transactions`
- **Read help**: Use `blazemeter_help` with action `read_help_info`, args: `{"category_id": "root_category", "subcategory_id": "guide", "help_id_list": ["performance-exclude-failed-transactions"]}`

---

## Compare Reports

Compare performance test reports to identify differences, including timeline comparisons, label filtering, and Request Stats comparisons. You can compare test reports to each other and examine any differences in reported results. In BlazeMeter, test reports for two different tests or two executions of the same test can be compared.

**Use when**: Comparing performance test reports to identify differences or analyzing changes between test runs.

### Set Up the Comparison

**Steps:**

1. Open one of the reports that you want to compare and click the button to the right of the test report name to open a sub-menu
2. Click the **Compare Report** option. You can also compare a report to a baseline. For more information, see [Baseline Comparison](skill-blazemeter-performance-testing://references/reporting.md). The **Add Test Runs to Compare** prompt window appears with the selected report
3. Select the project, the test and the test runs to compare. You can compare up to 10 test runs. Each time a new test report is added for tests that share a common name, all of the previous selected tests will appear in the **Select test runs to compare** field
4. Click **Compare**

You will be taken back to the **Test Runs Comparison** view, where you can see both tests:

The view will immediately populate with data from both reports, each represented by a different color line in each graph.

You can see graphs for Response time, users, latency, bandwidth, hits per second, errors per second and 90th percentile.

### Filter by Labels

If you want finer control over what report data is included in the comparison, you can use the **Select Labels** drop-down list to filter results based on the label names that you included in your test script. The default selection is ALL, meaning all labels or all report data.

### Adjust Resolution

You have the option to configure the graphs to measure by minutes or seconds using the **Resolution** drop-down menu.

### Export Graphs

You can export each graph individually to one of several formats by clicking the arrow icon in the top-right corner of the graph.

### Compare Request Stats Reports

You can generate a **Request Stats** comparison report in a tabular format to read and identify any deviations. This report can help you determine if there is any degradation in performance and pinpoint the specific endpoints affected.

This feature is available for both single tests and multi-tests. You can compare a **Request Stats** report to the baseline, the previous test run, or any other report. The compared report values appear in parentheses:

- **Green**: Greater than the original report
- **Red**: Less than the original report
- **Black**: Equal to the original report

**Steps:**

1. Open the required report and click **Request Stats**
2. Click **Compare**
3. To select a test for comparison, in the **Compare To** list, click one of the following options:
   - **Baseline** (if a baseline report exists)
   - **Previous Test Run** (if this test has run before)
   - **Any Other Report**
     When selected, the following lists display:
     - **Select a Project**. Click the required project
     - **Select a Test**. Click the required test
     - **Select a Test Run to Compare**. Click the required run of the selected test
4. In the **Data Comparison Mode** list, click one of the following options:
   - **Delta as Percentage (+50%)** – This option shows the difference between the compared values as a percentage
   - **Actual Values (300ms)** – This option displays the actual measured values for the metrics being compared
   - **Delta in Actual Units (+50ms)** - This option shows the difference between the compared values in the actual units used
5. Click **Compare**. The values of the compared report appear in the table in parentheses
6. When you have finished comparing reports, click **Close Configuration**

---

## Manage Tags for Tests

Add, remove, and manage tags for performance tests and reports to organize and search test data effectively. You can add tags to tests and reports to easily index data for search and to aggregate data based on the tags. Use tags to tie tests to a specific release, application, API, and so on, or to compare the same test in different projects.

**Important Notes:**
- Tags are shared across the workspace, and they are workspace-specific
- Only workspace managers can rename and delete tags
- If a tag is deleted, it is removed from all tests and reports
- Tag names are not case-sensitive
- Tag names must be unique

**Use when**: Adding, removing, or managing tags for performance tests and reports or organizing and searching test data effectively.

### Add Tags to Performance Tests and Multi-Tests

You can add tags to tests on the test configuration page. All workspace roles except 'Viewer' can add test tags. You can add up to 50 tags per test.

**Steps:**

1. Log in to BlazeMeter and navigate to the **Performance** tab
2. Click **Tests** and select a test from the drop-down list
3. Click the **Configuration** tab
4. In the test description area on the left, define your tag in the **Tags** text field. You can add more tags
5. Click **Create**

You've created a tag. The tag is shared between all Performance tests, multi-tests, and reports within the workspace. By default, reports will inherit the tags of the related test.

### Remove Tags from Performance Tests and Multi-Tests

You can remove tags from tests on the Test configuration page. All workspace roles except 'Viewer' can remove test tags.

**Steps:**

1. Log in to BlazeMeter and navigate to the **Performance** tab
2. Click **Tests** and select a test from the drop-down list
3. Click the **Configuration** tab
4. In the test description area on the left, view the tags in the **Tags** field and click the X next to the tag name

The tag is removed from the test. The tag is not deleted and therefore will not be removed from other tests, multi-tests, and reports that the tag is assigned to within the workspace.

For more information about how to delete a tag from a workspace, see "Manage Tags in a Workspace".

### Add or Remove a Tag in the Reports Page

Testers and managers can tag reports to tie them to specific releases, applications, APIs, and so on. You can add up to 50 tags per report.

**Steps:**

1. Log in to BlazeMeter and navigate to the **Performance** tab
2. Click **Reports** and select a report from the drop-down list
3. Navigate to the **Summary** tab
4. Perform the following actions as needed:
   - To add a new tag, enter the tag name into the **Tag** field
   - To remove a tag, click the X next to the tag name

### Manage Tags in a Workspace

Workspace managers and testers can manage tags at the workspace level.

**Steps:**

1. Log in to BlazeMeter and click the **Settings** button in the top right corner. The **Settings** page opens
2. Expand the **Workspace** drop-down list and click **Tags**. A list of all tags in the **Workspace** displays
3. Perform the following actions as needed:
   - To add a new tag, click the **Create New Tag** button in the top right corner. Then enter the tag name and click **Create**. The tags are sorted by name
   - To sort by any column, click the column header
   - To search tags, enter the tag name in the search field. Note: Search is not case-sensitive
   - To edit a tag name, click the edit button in the **Actions** column. Then edit the tag name and click **Apply**. The change will apply to all tests and results that the tag is assigned to
   - To delete a tag, click the bin icon in the **Actions** column, then click **Delete**. The tag will be deleted from the workspace and removed from all tests and reports the tag is assigned to

---

## Restore Archived Report

Restore archived performance test reports for accounts with unlimited data retention, accessing historical test data on-demand. This feature is only available for customers on accounts with **unlimited data retention**. [View plans](https://www.blazemeter.com/pricing/#pricing-details) or contact your account manager for more information.

BlazeMeter offers access to historical test data, including unlimited data retention on qualifying plans.

**Important Information:**
- Depending on your plan, you can keep and access test data for specific time periods, for example, from 1 week to 3 months, 6 months, 1 year, or customizable/unlimited
- For plans with unlimited data retention, test data older than 30 days will be archived but can be fetched on-demand
- Trends, histories, and other general test statistics are not affected by archiving
- For customers on accounts with **unlimited data retention**, baselines will not be archived
- If your account is moved from unlimited data retention to a tiered plan, there is a grace period of 2 months where you are still able to access your archived test data

**Use when**: Restoring archived performance test reports or accessing historical test data for accounts with unlimited data retention.

### Restoring a Report

Any team member except "Viewer" can restore an archived report. To restore an archived report, navigate to the report and click **Restore Report Data**.

Depending on the size of the report, the restore process may take up to a few minutes. While the restore runs in the background, you can continue to work. If you move to a different page or enter the report while it's restoring, a popup will notify you once the report data is ready.

**Important**: Restored reports are re-archived after 30 days.

---

## Share Reports

Share performance test reports with team members, including report sharing, access control, and collaboration features. It is very easy to share BlazeMeter reports. All you need to do is to click the shareable link and share it with anyone you want to see your report.

**Important**: No authentication is required to access any of BlazeMeter shareable report links.

**Use when**: Sharing performance test reports with team members or collaborating on test analysis.

### Share a Report

**Steps:**

1. Login to your BlazeMeter Account
2. In the **Performance** tab, select **Reports**. The most recent reports are shown on top
3. Click **Show all reports** and select a report to view its details
4. Click on the Actions menu in the top left section of the report
5. Click the **Share Report** button
6. Toggle **Sharing for this report is:** **ON**
7. Copy and share your report link

If the report has already been shared, the **Share Report** option in the report Action menu will appear in blue. If the report is currently not shared, the menu option will appear in black.

### Regenerate the Shareable Link

- The link is only active when **Sharing for this report is On**
- Turning **Sharing** off permanently deletes the current link. This also permanently removes access to the report via the link
- Toggling back to **Sharing for this report is On** generates a new sharing link

---

## Advanced Search

Use advanced search capabilities to find performance tests, multi-tests, and reports using multiple attributes and filters. BlazeMeter lets you search for performance tests, multi-tests, and reports using a variety of attributes, beyond just the test name.

**Use when**: Searching for performance tests, multi-tests, or reports using multiple attributes and filters or organizing and finding specific test data.

### Search Capabilities

With advanced search, you can:
- Customize fields displayed in the results to focus on relevant data
- Export results to a CSV file for sharing or further analysis outside BlazeMeter

You can search by the following attributes and their combinations (with an AND relationship):
- **Cloud Provider**
- **Create Date**
- **Created By**
- **Duration**
- **Locations**
- **Number Of Engines**
- **Project**
- **Run Date**
- **Tags**
- **Update Date**
- **Users**
- **Workspaces**

### Use Advanced Search

**Important Notes:**
- The order of filters on the screen matches the order in which the filters were selected
- Textual searches are **not** case sensitive
- By default, results are sorted by **Update Date** for tests and **Run Date** for reports, both descending and with latest results at the top. To sort by any of the columns displayed, click the column header
- Your search preferences (selected filters, columns, sorting) are saved in the browser for future use
- The relationship between the filters is always AND; for example, Created By = current user AND Create Date = this month
- The relationship between values within each (multi-select) filter is always OR; for example, Cloud Provider = (Google OR Amazon)
- The same filter can be added multiple times to create an AND relationship between its values; for example, Tags = Demo AND Tags = Iteration 1.1

**Steps:**

1. In **BlazeMeter**, go to the **Performance** tab
2. You can access Advanced Search by:
   - In the **Search** box, click **More Search Options**
   - From the **Tests** or **Reports** drop-down lists, click **Go to Advanced Search**
3. Select the **Workspace**. Your advanced search will only search in the selected workspace
4. From the **Add Filter** drop-down list, select attributes for filtering
5. For each selected filter, a new drop-down menu appears. Expand it and specify your selection

By default, the results show the following columns:
- **Tests, Multi-Tests** - Name, Type, Project, Locations, Users, Created By, Create date, Update date
- **Reports** - Name, Type, Project, Locations, Users, Executed by, Run date

You can see reports that are currently running - they have a **Stop Test** sign next to them.

6. (Optional) To edit columns, click the **Edit Columns** button in the top right corner
7. (Optional) Click the test (or multi-test or report) name to navigate to that particular test (or multi-test or report). You will be taken to the test detail page
8. (Optional) Run a test, a multi-test, or re-run a report from the search results. Hover over the test and in the **Actions** column, click **Run Test** or **Stop Test**

### Refine Search Results

BlazeMeter provides search filtering capabilities that allow you to refine results by requiring all selected values to match in multi-selection filters, such as **Tags**. This feature helps you locate a very specific set of test runs or reports.

You can select the same filter multiple times, with each instance using a different value. This creates an **AND** relationship, narrowing the results to include only items matching all selected values.

**Benefits:**
- Enables more precise filtering for detailed test and report analysis
- Streamlines the process of finding specific results that match multiple criteria
- Avoids sifting through irrelevant test runs that only match one of the tags

This feature is particularly useful for teams managing extensive test data, as it simplifies the identification of tests and reports relevant to complex requirements. This saves time and ensures they analyze only the data they need to make informed decisions about test performance and coverage.

**Steps:**

1. In BlazeMeter, navigate to the **Performance** tab
2. In the search bar, click **More search options**. Alternatively, access **Advanced Search** from the **Tests** or **Reports** drop-down menus
3. Select the **Workspace**. Your advanced search will only search in the selected workspace
4. Click **Add Filter** and select the filter you want to apply (for example, **Tags**)
5. Select a value for the filter from the dropdown
6. Add the same filter again with a different value
7. View the refined results in the search table

**Example**: A tester is tasked with analyzing performance test results that meet multiple specific criteria. They want to identify all test runs tagged with both *"Release 1.1"* (to focus on a specific product version) and *"Flights App"* (to narrow the results to a particular application or module). They add the **Tags** filter twice, selecting *Release 1.1* in one instance and *Flights App* in the other. The results display only those items tagged with both values.

---

## Intro to Reporting

After you run a Performance test, a report with detailed data is available. Understand Performance Test Reporting in BlazeMeter, including Summary, Timeline, Request Stats, Engine Health, Errors, Logs, and Original Test Configuration reports.

**Use when**: Understanding Performance Test Reporting in BlazeMeter, viewing test reports, or working with Summary, Timeline, Request Stats, Engine Health, Errors, Logs, and Original Test Configuration reports.

### Overview

After you run a Performance test, a report with detailed data is available. The report provides comprehensive insights into test execution, performance metrics, errors, and system health.

### View the Test Report

Follow these steps:

1. In the **Performance** tab, select **Reports**. The drop-down list shows reports for **Recent Test Runs**
2. Select either:
   - One of the recent reports. The report will open
   - **Show all reports**. A list of all reports will appear on the left. To find your report, start typing the name in the search box and then select the report
3. A test report overview opens. If your report is set as a baseline, you will see it below the name
4. Select each of the tabs to view more report details

### Report Types

- **Summary Report**: The main dashboard view of your test. Quickly access data such as Max Users, Average Throughput, Errors, Average Response Time, and Average Bandwidth
- **Timeline Report**: Displays the various types of Key Performance Indicators (KPIs) within one graph so you can easily visualize certain events that might have occurred throughout the test
- **Request Stats Report**: Displays the KPIs or metrics for each element of a test script, allowing you to drill down and review the statistics (stats) for how every single element of your test performed
- **Engine Health Report**: Displays performance indicators received from the test engines. Engine Health indicates whether the test infrastructure itself could be the cause of bottlenecks or the errors which are appearing in other reports
- **Errors Report**: Displays all errors received during the test run, categorized by labels (pages) and error types
- **Logs Report**: Displays the logs of each engine used during the test run. You can display and filter engines and logs, and then select multiple logs for download
- **Original Test Configuration Report**: Displays how your test was configured at the time the test covered by the report ran. This tab is especially useful if you have updated the test configuration since the report was run, as the details here will reflect the configuration prior to those updates. For more information, see [Original Test Configuration Report](skill-blazemeter-performance-testing://references/reporting.md)

### Quick Search for Reports

You can quickly search within the tab for reports by their name, without having to go through the **Show All Reports** side bar.

Follow these steps:

1. Navigate to the **Performance** tab
2. In the search field, enter a report name. The search is not case sensitive. The field displays top 5 recently executed reports that match your search
3. To see the full list of results, select **Show All Results**

The side bar with Reports shows on the left. The reports are sorted by execution date.

### Report Header Actions

In the header area of performance test reports, you can perform various actions to interact with the report and access different features. Some common actions include:

- **Navigate** — You can navigate between different sections or tabs of the report, such as Timeline, Request Stats, Engine Health, Errors, and Summary
- [Filtering Report Data](skill-blazemeter-performance-testing://references/reporting.md) — You can adjust the filter to manage the data displayed in the report
- [Set a Report as a Baseline](skill-blazemeter-performance-testing://references/reporting.md) — You can select or set a baseline for comparison purposes. This helps you compare the current test run with a previous one to identify performance improvements or regressions
- [Sharing Reports](skill-blazemeter-performance-testing://references/reporting.md)
- [Comparing Reports](skill-blazemeter-performance-testing://references/reporting.md)
- [Generate an Executive Summary (Printable Report)](skill-blazemeter-performance-testing://references/reporting.md)

---

## Summary Report

The Summary Report is the main dashboard view of your test while the test is running and after it has finished. The Summary Report will appear as soon as your test starts to collect data.

**Use when**: Viewing the main dashboard of your performance test, accessing key statistics, or generating printable executive summaries.

### View Summary Report

Follow these steps:

1. In the **Performance** tab, select **Reports**. The most recent reports are shown on top. You can enter a report name in the search field to view the top 5 reports sorted by execution date
2. Click **Show all reports**, then select a report to view its details. A test report overview opens, with the Summary Report as the default view

### Summary Panel

The summary panel at the top of the report shows key statistics of the test compared with the baseline. For more information about baseline, see [Baseline Comparison](skill-blazemeter-performance-testing://references/reporting.md).

- **Max Users**: The maximum number of concurrent users generated at any given point in the test run is determined according to one-second intervals. First, the maximum number of concurrent users per second is calculated. Then the maximum number of users per minute is calculated by performing a Max Users calculation on all the values per second. Max Users is **NOT** the number of Total Users, only the total of users who ran simultaneously at any given moment. As a result, Max Users may not match your Total Users, which may be significantly higher.

  **Example**: At location A, user A sends a request at 20:15:00. At location B, user B sends a request at 20:15:01. The maximum number of concurrent users at 20:15:00 is 1. The maximum number of concurrent users at 20:15:01 is 1. The maximum number of concurrent users at 20:15 is 1 because first the maximum number of concurrent users per second is calculated, then the maximum value per minute is calculated. In this case, it is also 1.

- **Average Throughput (Hits/s)**: The average number of HTTP/s requests per second that are generated by the test. Throughput for individual transactions (labels) is available on the [Request Stats](skill-blazemeter-performance-testing://references/reporting.md) tab (hits/s).

  **Note for JMeter tests**: BlazeMeter counts unique requests that appear in the JTL file generated during the test. This means that if only high-level requests are present in the JTL file, the Hits/s figure relates only to the high-level requests. If while configuring the test, you select to include sub-samples in your runs, then HITS/s represents all high-level requests and sub-samples (images, CSSs, JSs and so on).
- **Errors**: The ratio of bad responses out of all responses received
- **Average Response Time**: The average amount of time from the first bit sent to the network card to the last byte received by the client
- **90 Percentile of Response Time**: The top average value of the first 90% of all samples. Only 10% of the sample is higher than this value
- **Average Bandwidth (MB/s)**: The average bandwidth consumption in MegaBytes per second generated by the test

### Overview

The overview section shows key configurations with the main categories of response codes received during the test run. This really helps you grasp the general purpose and performance.

- The Test Duration (HH:MM:SS)
- The Test Start & End Times
- The Test Type - JMeter Test, Multi-Test, URL/API Test, Webdriver Test
- Locations: The geo-locations the load has originated from
- Response Codes: A breakdown of the HTTP response status codes received during the test run
- Internal notes about the report

### Graphs

There are two graphs that indicate the key performance metrics and their trends throughout the duration of the test.

- **Load Graph**: Shows the maximum number of users versus hits per second versus errors rate. In the example above, you can see that while the load increased gradually until it reached its maximum, the hit/s increased rapidly and remained relatively high for the duration of the test and the error rate stayed at 0%
- **Response Time Graph**: Shows the maximum number of users vs response times, revealing how the size of the load affects the response times

### Generate a Printable Executive Summary (New)

You can directly generate a customizable and printable version of a report using the **Executive Summary (New)** option in the **Actions** menu.

The **Executive Summary** is a test report summary you can send to managers or other team members.

This report includes all the components of the performance report, with the filters and view preferences applied by users (see [Filtering Report Data](skill-blazemeter-performance-testing://references/reporting.md)).

**Examples**: The **Timeline** component in the **Executive Summary** displays the labels and KPIs selected in the **Timeline** tab, the selected time range, graph resolution and additional customized charts. The **Request Stats** component in the **Executive Summary** includes the data as displayed on the **Request Stats** tab, including filters on labels, baseline comparison data, filtered by time, and so on.

**Follow these steps:**

1. Click the three dots next to your test report and select **Executive Summary**. A new page with the test report displays
2. (Optional) At the top of the report, you can write your own summary and conclusions

The printable executive summary includes:
- Test setup details
- Timeline graph reflecting the **Timeline** tab, including selected labels and KPIs, multiple charts, and so on
- Summarized request stats report, filtered by the same filter and view preferences used on the **Request Stats** tab
- Summarized engine health report, filterable by labels
- Summarized error report with response codes, grouped by labels

### Generate a Printable Executive Summary (Classic)

You can generate a printable version of a report using the **Executive Summary (Classic)** option on the **Actions** menu.

**Follow these steps:**

1. Click the three dots next to your test report and select **Executive Summary (Classic)**. A new page with the test report displays
2. (Optional) To customize your report logo, click the edit icon on the logo area. If you have customized your account logo in the [Account Settings](skill-blazemeter-administration://references/workspaces-projects.md), your custom logo will display
3. (Optional) At the top of the report, you can write your own summary and conclusions

The printable executive summary includes:
- Top 5 slow responses
- Top 5 errors
- Test setup details
- Graphs presenting users, response times, and hits per second
- Summarized aggregate report, filterable by labels
- Summarized error report

---

## Timeline Report

The Timeline report shows various types of KPIs within one graph so you can easily visualize certain events that might have occurred throughout the test.

**Use when**: Visualizing KPIs throughout the test duration, viewing KPIs by labels, viewing KPIs from APM integrations, or analyzing anomalies in response times.

### Explanation of the KPIs

- **ANOMALIES - RESPONSE TIME**: Shows unexpected or irregular patterns in the time it takes for the application under test to respond to user requests or actions. See [View Anomalies](skill-blazemeter-performance-testing://references/reporting.md)
- **VIRTUAL USERS**: Shows how many virtual users are currently active
- **HITS (Hits per second)**: The number of HTTP/s requests per second that are generated by the test
- **RESPONSE TIME**: The amount of time from the first byte sent to the server to the last byte received on the client side
- **CONNECT TIME**: The time it takes to establish the TCP connection, including host name resolution and SSL handshake. It depends on DNS server speed, network latency, and DNS caches. It is recorded in the report every time an HTTP or HTTPS connection was initiated to communicate with the server
- **LATENCY TIME**: The time from sending the request and processing it on the server side, to the time the client received the first byte
- **BYTES**: The average bandwidth consumption that's generated by the test per second
- **ERRORS**: Show how many errors occurred
- **RESPONSE CODES**: The response codes received

### View Timeline Report

As soon as your test starts to collect data, the Timeline Report graph shows.

Follow these steps:

1. In the **Performance** tab, select **Reports**. The most recent reports are shown on top. You can enter a report name in the search field to view the top five reports sorted by execution date.
2. Click **Show all reports** and select a report to view its details. A test report overview opens.
3. Click the **Timeline Report** tab.

In the Timeline Report window, the KPI selection panel is on the left.

### View KPIs by Labels

To view KPIs by labels, click the arrow next to each KPI and select the required label from the different options that open.

Example: Hits of **Login** in the following image.

You can see the KPIs that are available for every label in your test.

You can filter the view of KPIs to save a lot of time and manual effort if you have some tests with many labels. If you type a KPI name in the **KPI Selection** textbox and click **Select All**, all the labels under this KPI are selected. The **KPI Selection** textbox is case-insensitive.

### Response Time Filter

The response time KPI includes many metrics, such as average (avg), maximum (max), minimum (min), median, 90th/95th/99th percentile (P90/P95/P99), and so on. If you type a KPI name and a metric in the **KPI Selection** textbox and click **Select All**, all the labels under this KPI, together with the specified metric are selected.

If you type *response time* in the **KPI Selection** textbox, the Response Time KPI expands. By default, the **Avg** metric is selected for **All**.

If you click **Select All**, the **Avg** metric checkbox is selected for all labels, and the graph updates accordingly.

You can type the name of any available metric in the **KPI Selection** textbox. If you type *max* and click **Select All**, the **Max** metric is selected for all labels.

Let's say that at the **All** level, **Avg** and **Min** are selected. You type *response time max* into **KPI Selection** and click **Select All**. At the **All** level, **Avg** and **Min** remain selected.

To clear your selection, click **Unselect All**. To restore the default settings, click **Reset to Default**. You can only enter one metric name at a time in the **KPI Selection** textbox. A new selection adds to an existing selection and does not remove it. Let's say that you enter a metric name in the **KPI Selection** textbox such as *response time max* and click **Select All**. You then replace response time max with response time min and click **Select All** again. For each label, all the **Max** checkboxes remain selected, and all the **Min** checkboxes are selected.

### View KPIs from APM Integrations

KPIs from [APM Integration](skill-blazemeter-performance-testing://references/advanced-features.md) profiles that you included in your test configuration will appear at the bottom of the list, after the built-in KPIs.

### Download Timeline Report KPI Data

You can download the KPI data that BlazeMeter uses to build the Timeline Report via a call to the [Time-Series Data API](https://help.blazemeter.com/apidocs/performance/masters_time_series_data.htm):

```
https://a.blazemeter.com/api/v4/data/kpis?interval={{seconds}}&kpis[]={{kpi}}&ops[]={{operand}}&master_ids[]={{masterId}}&labels[]={{label}}
```

The above API call includes the following options:

- **{{seconds}}**: The number of seconds between each data point generated
- **{{kpi}}**: The KPI you want to view. This parameter can be used multiple times in the same request to cover as many KPIs as you are interested in. The list of values is as follows:
  - `na`: Number of Active threads
  - `ec`: Error Count
  - `ct`: Connect Time
  - `t`: Response time
  - `lt`: Latency
  - `ts`: Time stamp in epoch format
  - `n`: Number of hits per second
  - `by`: Bytes per second
- **{{operand}}**: This is how you find the average, minimum, or maximum values for applicable KPIs. The list of acceptable values is as follows:
  - `avg`
  - `min`
  - `max`
- **{{masterId}}**: The master ID of the report you want to see the data from
- **{{label}}**: The label ID of the label you're interested in

Use of the BlazeMeter API requires authentication per the instructions found in the article [Authorization](skill-blazemeter-api-reference://references/authentication.md).

### Sliding Windows

You can evaluate failure criteria by enabling 1-minute sliding windows when creating and editing pass and fail criteria in tests. In the event that failure criteria are triggered, the time frame during which violations occurred is shown in the Timeline report as a sliding window.

Sliding windows offer the following benefits:
- Prompt detection of violations against the performance thresholds set in the failure criteria, enabling quick action to address potential issues
- Proactive approach to test evaluations by offering insights early and frequently during test executions
- Clear reporting in the **Timeline** report helps you to easily identify and investigate specific periods of concern, streamlining the troubleshooting process

To enable sliding windows, create a test, enable failure criteria, and select the **1-min slide window eval** option for the criteria required.

You can set the **1-min slide window eval** option for all failure criteria simultaneously so that all failure criteria can stop a test.

For more information, see [Failure Criteria](skill-blazemeter-performance-testing://references/advanced-features.md).

**How it works**: Sliding windows evaluate performance against failure criteria in real time, checking at regular intervals during the test duration. For instance, if a failure criterion is set, such as an average response time exceeding 500 ms, BlazeMeter continuously checks the performance of the preceding minute.

At specified intervals, such as the first minute (01:00), BlazeMeter evaluates the performance of the preceding minute, creating segments (such as 00:00-01:00) for analysis. This process is repeated every few seconds, enabling a detailed examination of performance during the test.

If the designated threshold is exceeded at any checkpoint, BlazeMeter promptly flags it as a violation. This real-time detection allows immediate awareness of issues as they occur during the test, facilitating quicker identification and response to potential problems.

In the event of violations, the **Timeline** report highlights these periods with a distinct red rectangle. This visual representation makes it easier for you to identify and investigate areas of concern, allowing for efficient troubleshooting and resolution.

### Use Multiple Charts

You can add charts to the charts that displays by default. This feature helps you to efficiently analyze reports and identify correlations and anomalies. This capability avoids the need to continuously toggle between selecting and deselecting labels and KPIs, resulting in a time-consuming process. You can customize additional charts, assign names to them, and arrange them in their preferred layout.

Multiple timeline charts let you distribute labels and KPIs across different charts on the page, facilitating a more focused and efficient analysis. This feature enables faster comprehension of data, clearer visualization of trends, and easier highlighting and sharing of conclusions. By leveraging multiple charts in the **Timeline** tab, you can enhance your performance testing workflows, ultimately leading to more insightful and actionable outcomes.

You can add charts as follows:
- Click **Duplicate Chart** next to an existing chart.
- Click **Add Chart** below an existing chart.

### Link to Errors Tab

You can click links from the **Timeline Report** directly to the **Errors** tab where errors are grouped by the relevant label. This process is designed to aid you in identifying and resolving issues efficiently.

Each data point in the **Errors** graph includes a deep link to the **Errors** tab. Clicking on a data point in the Errors graph opens the Errors tab with the selected label expanded.

If BlazeMeter cannot determine a specific label (for example, due to overlapping graphs), the aggregated **All** label is expanded by default on the **Errors** tab.

**Benefits:**

**Automated navigation.** You do not need to manually navigate to the **Errors** tab, remember the label name, and locate it in a long list, which is potentially cumbersome and time-consuming. In tests with numerous labels, automated navigation can be helpful in identifying and tracking errors for a specific label.

**Example:**

1. You observe that 10 minutes into the test, label "A" starts returning errors on the **Timeline Report** tab.
2. You hover over the errors graph for label "A" and click on the link in the corresponding data point.
3. The **Errors** tab opens with errors grouped by label. Label "A" is expanded by default, displaying all errors associated with this label.

**Jump to next section:**
- [Request Stats Report](skill-blazemeter-performance-testing://references/reporting.md)

### View Anomalies

In the Timeline report, anomalies are highlighted on the graph so that users can easily see which label(s) show abnormal behavior, and when exactly the anomaly happened. With that information surfaced, you can focus on the API/endpoint that needs investigation, see what errors occurred during that time, and investigate what else happened during that time that might have caused the issue.

You can view anomalies in **Timeline** reports to help identify unexpected performance deviations during test runs. This feature uses a statistical model that requires a minimum sample size of 60 data points to detect anomalies effectively.

**This feature is available exclusively for Enterprise plans ("Unleashed").**

Free, Basic, and Pro accounts do not have access to this feature. To upgrade to an Enterprise plan, contact your BlazeMeter account manager or email [sales-blazemeter@perforce.com](mailto:sales-blazemeter@perforce.com).

Anomalies are shown for the following metrics:
- Average Response Time (Avg)
- 90% Response Time (P90)
- 95% Response Time (P95)
- 99% Response Time (P99)
- Median Response Time (Median)

Anomalies are detected during master test runs but are highlighted in the Timeline chart only after the run ends to ensure accurate and comprehensive analysis. The detection process is optimized to prevent any degradation in BlazeMeter's performance.

If **Scenario**, **Location**, or **Transaction** filters are applied, anomalies are hidden because detection is not performed on filter permutations. However, anomalies within the selected time frame are displayed when Time filters are applied.

Anomalies are detected and displayed for multi-test reports, ensuring comprehensive analysis across various tests.

Account owners can determine whether anomaly detection is available. For more information, see [Manage Environments](skill-blazemeter-administration://references/workspaces-projects.md).

**Steps:**

1. In the **Performance** tab, select **Reports**.
2. Navigate to the report of the required master test run.
3. Check the anomalies notification at the top of the screen. If anomalies were not detected, a notification stating *No Anomalies Detected* displays. If anomalies were detected, a notification stating *Anomalies Detected* displays.
4. To view any detected anomalies, navigate to the **Timeline Report**.
5. Navigate to **Anomalies - Response Time** in the KPI selector panel and select the required KPIs. The anomalies for the selected KPIs are highlighted in the timeline chart.

If you're unsure why certain data points were flagged as anomalies, try adjusting the chart resolution to 1-second intervals. At this resolution, deviations from the mean become more visible, allowing you to see patterns across groups of data points and better understand what triggered the anomaly detection. This adjustment provides a clearer view of data fluctuations, helping you interpret detected anomalies more accurately.

---

## Errors Report

The Errors tab contains the errors that were received by the web-server under the test as a result of HTTP request. The report displays all errors received during the test run, categorized by labels (pages) and error types.

**Use when**: Viewing errors received during test runs, analyzing error types, viewing response bodies of failed samples, or troubleshooting test failures.

### View Errors Report

Follow these steps:

1. In the **Performance** tab, select **Reports**. The most recent reports are shown on top.
2. Click **Show all reports** and select a report to view its details.
3. Click the **Errors** tab. You can see all errors received during the test run.
4. (Optional) Move the timeline sliders or [Filter by Date and Time](skill-blazemeter-performance-testing://references/reporting.md) to narrow down the time. The results for your selected time period will show in the results table.
5. Group errors by: Label, Response Code, or Assertion Name.

### Types of Errors

We report these types of errors:
- Top requests
- Assertions
- Failed embedded resources

For each error, the following information displays:
- Response code
- Response message
- Number of failed requests

### View the Response Body

You can access the response data of failed samples on the **Errors** tab. This feature can help you:
- Quickly identify problems
- Save time searching through logs
- Instantly see issues during test runs
- Avoid excessive memory usage on local machines

Admins and account owners can enable or disable this feature on the account settings page. For more information, see [Manage Environments](skill-blazemeter-administration://references/workspaces-projects.md).

**Key Features:**
- **First 10 Unique Errors**: Only the first 10 unique error response bodies per scenario or test are displayed. This constraint is designed to help manage storage and performance
- **Live Display**: The response data is available while the report is running, not just post-completion
- **Scope**: This feature is available for all performance tests, shared reports, external reports, and multi-test reports

**Steps:**

1. Open a test report and navigate to the **Errors** tab.
2. In **Group errors by**, select **Label**, **Response Code**, or **Assertion Name**. The **Response Codes** table opens, containing the following columns: **Code** - Lists the specific error codes detected. **Description** - Provides a brief description of each error. **Response Body** - Contains links labeled `Response` that you can click to view the detailed error response body. **First Occurrence** - Shows the time stamp of the earliest occurrence of an error or error response body during a test. This information can help you identify and address the root cause of issues by showing the initial instance of errors. **Number of Errors** - Shows the number of times each error occurred during the test.
3. In the **Response Body** column, click any table cell containing a **Response** link. If response data exists, one or more **See Response** links are shown.
4. To open a detailed view of the specific error response body, click **See Response**. The data is shown in raw HTML format. The dialog contains the following fields: **Label** - Indicates the type or category of the response. **Code** - Displays the specific error code. **First Occurrence** - Shows the time stamp of the earliest occurrence of an error or error response body during a test. This information can help you identify and address the root cause of issues by showing the initial instance of errors. **Count** - Indicates the total number of times the error occurred. **Description** - Contains a brief description or message associated with the error. **Response Body** - Displays the detailed error response body, including the stack trace.
5. (Optional) Click **Copy to clipboard** to copy the response body.

**Jump to next section:**
- [AI Log Analysis Report](skill-blazemeter-performance-testing://references/reporting.md)

---

## Request Stats Report

The **Request Stats** report shows the key performance indicators (KPIs), or metrics, for each element of a test script, so that you can drill down and review the performance statistics (stats) for each element of your test.

**Use when**: Viewing KPIs for each element of a test script, drilling down into performance statistics, filtering by labels, comparing with baseline, or downloading report data.

### View the Request Stats Report

Follow these steps:

1. On the **Performance** tab, select **Reports**. The most recent reports are shown on top.
2. Click **Show all reports** and select a report to view its details.
3. Click the **Request Stats** tab.
4. (Optional) To specify the time period of the report, move the timeline sliders or [Filter by Date and Time](skill-blazemeter-performance-testing://references/reporting.md). The results for your selected time period are displayed in the results table.
5. (Optional) To define a test run as a baseline for comparison, turn on the **Show Baseline Comparison** toggle and specify the baseline. You can compare subsequent test runs to the baseline. For more information, see [Baseline Comparison](skill-blazemeter-performance-testing://references/reporting.md).
6. To filter statistics based on labels, in the **Filter By Label** list, click all labels that you want to display. If you select **ALL**, values for all requests made during the test are displayed. If you used your own JMeter script, this table displays the labels you used in your script.
7. Click **Apply**.
8. (Optional) To specify the KPIs to be displayed in the table, click **Edit Columns**. Examine the list of available KPIs. Drag and drop the columns you want to display into the desired order.

**The available KPIs include:**
- **Element Label** The name of the HTTP Request from the JMeter script.
- **Avg. Response Time (ms)** The average response time for executed requests. While the test is running, this KPI displays the average of the requests already executed, and the final value once test execution is finished.
- **90% line (ms)** 90th percentile. 90% of the samples were smaller than or equal to this time.
- **99% line** **(ms)**99th percentile. 99% of the samples were smaller than or equal to this time.
- **Max Response Time** **(ms)**The longest time for the samples with the same label.
- **Error Percentage** The error rate per label. While the test is running, this KPI displays a value based on samples already completed, and a final value after completion of test execution.
- **StDev (ms**) The standard deviation (a measure of variation) of the sampled elapsed time.
- **Duration (hh:mm:ss)** The sum of the duration for all samples with the specified label or labels.
- **# Samples** The total number of samples executed.
- **Avg. Hits/s** The number of requests made per second. When the throughput is saved to a CSV file, the value is expressed in requests/second. For example, 30.0 requests/minute is saved as 0.5. When the test is done, the KPI displays the throughput for the duration of the entire test.
- **95% line** **(ms)**95th percentile. 95% of the samples were smaller than or equal to this time.
- **Min Response Time** **(ms)**The shortest time for the samples with the same label.
- **Avg. Bandwidth (Kbytes/s)** The volume of traffic in kilobytes per second (KBps).
- **Avg. Latency** The average latency for executed requests.
- **Error Count** The number of errors, including response codes and JMeter assertions.
- **Median Response Time (ms)** 50th percentile. 50% or half of the samples are smaller than the median, and half are larger.


### Aggregated Labels

The **AGGREGATED LABELS** row appears in the **Request Stats** report if the number of labels per engine or session is more than 300.

The report displays the first 300 element labels executed by your JMeter test. If your JMeter test script has more than 300 labels, the **AGGREGATED LABELS** row appears directly below the **ALL** row. The **AGGREGATED LABELS** row shows only the sample count for those labels beyond the first 300 displayed:

### Why Are Aggregated Labels Important?

Creating tests with hundreds of labels is not considered to be a best practice. However, sometimes it is unavoidable. Consider a case where you want to test an end-to-end user flow in your application. You can use a multitest with multiple scenarios, each covering a different part of the flow.

The **Request Stats** report provides a summary of the test results in a tabular format. The table includes an **Element Labels** column that displays the name of the sampler or transaction that was executed during the JMeter test.

Aggregated labels ensure that your report remains manageable and comprehensible, even when dealing with many labels. Aggregated labels help prevent data overload and maintain the clarity of the report, so that you can focus on the most critical aspects of your performance test.

### How to Interpret Aggregated Labels

When you see the **AGGREGATED LABELS** row in your report:
- The first 300 labels executed by your test script are displayed normally.
- The **ALL** label includes all labels (300+), incorporating them into its calculations.
- The **Request Stats** report can be filtered by scenario and location, but the report displays only a maximum of 300 labels per engine.
- If at least one test in a multitest has more than 300 unique labels, the report will show the **AGGREGATED LABELS** row. If none of the tests in the multitest have more than 300 unique labels, but there are more than 900 unique labels across all tests, you will NOT see the **AGGREGATED LABELS** row. The **ALL** and **AGGREGATED LABELS** rows are not included in the 900 report label limit.

**Example**

In a multitest with two scenarios, where one generates labels 1-300 and the other generates labels 301-600, all 600 labels will be visible in the report.

### Download a Report in CSV Format

To download the aggregated report data for the Request Stats Report in CSV format, click **Download CSV**:

### Download a Report in CSV Format via the API

Report data in CSV format can also be downloaded via [BlazeMeter's API](https://help.blazemeter.com/apidocs/).

After a test is executed, you can run an API call to download the file where <master_id> specifies the unique identifier for the test execution, <id> specifies your BlazeMeter user ID, and <secret> specifies your BlazeMeter API key.

```
curl -o report.csv -X GET https://a.blazemeter.com/api/v4/masters/<master_id>/reports/aggregatereport/data.csv --user '<id>:<secret>'
```

### Link to the Timeline Report

When analyzing a performance test report, you can explore specific details, particularly regarding the performance of individual labels during the test run.

On the **Request Stats** tab, each cell in the following metric columns contains a clickable link to the **Timeline Report**: **Avg. Response Time (ms)**; **Avg. Hits/s**; **90% line (ms)**; **95% line (ms)**; **99% line (ms)**; **Min Response Time (ms)**; **Response Time (ms)**; **Avg. Bandwidth (KBytes/s)**.

With the linking capability, you can transition from a high-level overview of test results to a detailed, time-based analysis, which can help in anomaly detection. The relevant graph, corresponding to the selected cell, is automatically displayed in the main chart at the top of the **Timeline Report** tab.

**Suggested Flow:**

1. On the **Request Stats** tab, access your test results and locate the **Error Percentage** column
2. Look for labels with high error percentages which might indicate performance issues
3. Click the required metric cell to open the **Timeline Report**
4. Analyze the time-based metrics displayed for the selected label to pinpoint when and why errors occurred

**Benefits:**

- **Targeted troubleshooting**: Drill down from aggregate error data to specific time-based details, making it easier to understand the context and causes of performance issues. By viewing errors in the context of the overall performance timeline, you can better identify the root causes of issues, such as specific test phases, traffic patterns, or external factors
- **Time-based analysis**: The **Timeline Report** provides a visual representation of performance metrics over time, so that you can identify trends, spikes, and drops in performance that correspond to specific events or changes in the test environment
- **Direct path to data**: Instead of manually navigating through different reports, deep links provide a direct path to the most relevant data

**Example:**

Imagine you are running a performance test and notice that the error percentage for a particular label is unusually high. By clicking on the required metric cell for that label, you open the **Timeline Report** and see that errors began to spike 10 minutes into the test. By analyzing the timeline, you discover that a backend service failed at that point, causing the errors. This linkage helps you identify and resolve the issue.

### Link to the Errors Tab

**Steps:**

1. On the **Request Stats** tab, access your test results and locate the **Error Percentage** column.
2. Look for cells with high error percentages which might indicate performance issues.
3. Click the required cell to open the **Errors** tab.

The selected label is expanded by default.

**Jump to next section:**
[Engine Health Report](skill-blazemeter-performance-testing://references/reporting.md)

---

## Advanced Search

BlazeMeter lets you search for performance tests, multi-tests, and reports using a variety of attributes, beyond just the test name.

**Use when**: Searching for tests, multi-tests, or reports using multiple attributes, filtering by various criteria, customizing result columns, or exporting search results to CSV.

### Use Advanced Search

With advanced search, you can:
- Customize fields displayed in the results to focus on relevant data
- Export results to a CSV file for sharing or further analysis outside BlazeMeter

You can search by the following attributes and their combinations (with an AND relationship):
- Cloud Provider
- Create Date
- Created By
- Duration
- Locations
- Number Of Engines
- Project
- Run Date
- Tags
- Update Date
- Users
- Workspaces

**Search Behavior:**
- The order of filters on the screen matches the order in which the filters were selected
- Textual searches are **not** case sensitive
- By default, results are sorted by **Update Date** for tests and **Run Date** for reports, both descending and with latest results at the top. To sort by any of the columns displayed, click the column header
- Your search preferences (selected filters, columns, sorting) are saved in the browser for future use
- The relationship between the filters is always AND; for example, Created By = current user AND Create Date = this month
- The relationship between values within each (multi-select) filter is always OR; for example, Cloud Provider = (Google OR Amazon)
- The same filter can be added multiple times to create an AND relationship between its values; for example, Tags = Demo AND Tags = Iteration 1.1

**Steps:**

1. In **BlazeMeter**, go to the **Performance** tab
2. You can access Advanced Search by:
   - In the **Search** box, click **More Search Options**
   - From the **Tests** or **Reports** drop-down lists, click **Go to Advanced Search**
3. Select the **Workspace**. Your advanced search will only search in the selected workspace
4. From the **Add Filter** drop-down list, select attributes for filtering
5. For each selected filter, a new drop-down menu appears. Expand it and specify your selection. By default, the results show the following columns:
   - **Tests, Multi-Tests**: Name, Type, Project, Locations, Users, Created By, Create date, Update date
   - **Reports**: Name, Type, Project, Locations, Users, Executed by, Run date. You can see reports that are currently running - they have a **Stop Test** sign next to them
6. (Optional) To edit columns, click the **Edit Columns** button in the top right corner
7. (Optional) Click the test (or multi-test or report) name to navigate to that particular test (or multi-test or report). You will be taken to the test detail page
8. (Optional) Run a test, a multi-test, or re-run a report from the search results. Hover over the test and in the **Actions** column, click **Run Test** or **Stop Test**

### Refine Search Results

BlazeMeter provides search filtering capabilities that allow you to refine results by requiring all selected values to match in multi-selection filters, such as **Tags**. This feature helps you locate a very specific set of test runs or reports.

You can select the same filter multiple times, with each instance using a different value. This creates an **AND** relationship, narrowing the results to include only items matching all selected values.

**Benefits:**
- Enables more precise filtering for detailed test and report analysis
- Streamlines the process of finding specific results that match multiple criteria
- Avoids sifting through irrelevant test runs that only match one of the tags

This feature is particularly useful for teams managing extensive test data, as it simplifies the identification of tests and reports relevant to complex requirements. This saves time and ensures they analyze only the data they need to make informed decisions about test performance and coverage.

**Steps:**

1. In BlazeMeter, navigate to the **Performance** tab
2. In the search bar, click **More search options**. Alternatively, access **Advanced Search** from the **Tests** or **Reports** drop-down menus
3. Select the **Workspace**. Your advanced search will only search in the selected workspace
4. Click **Add Filter** and select the filter you want to apply (for example, **Tags**)
5. Select a value for the filter from the dropdown
6. Add the same filter again with a different value
7. View the refined results in the search table

**Example:**

A tester is tasked with analyzing performance test results that meet multiple specific criteria. They want to identify all test runs tagged with both *"Release 1.1"* (to focus on a specific product version) and *"Flights App"* (to narrow the results to a particular application or module).

They add the **Tags** filter twice, selecting *Release 1.1* in one instance and *Flights App* in the other.

The results display only those items tagged with both values.

---

## Monitoring and Post-Test Analysis

Now that we've covered the various elements and best case practices of setting up and running effective performance tests, we'll dive into monitoring them properly and how to conduct post performance test analysis.

**Use when**: Monitoring test execution, analyzing individual components, conducting post-test analysis, or understanding monitoring tools and counters.

### Monitoring And Its Importance

Monitoring is important to analyzing individual component(s) of the application.

During performance testing virtual users create load on an application, and the tester's direct involvement in process is less (and tools involvement is more). Thus, it is important to monitor the system and its individual components.

**What should you monitor during a test?**

- The application server and all the activities happening
- The database server and all the activities happening
- The Web server and all the activities happening
- The network and bandwidth availability
- All client side activities
- Firewalls
- Any intrusion software or solution if used

**The importance of monitoring:**

There are three main reasons monitoring is critical to performance testing:
- It enables you to pin point any issues quickly
- It provides sponsors a true picture of the capacity of the existing infrastructure and helps them make decisions on weather to invest more or decrease investments in the infrastructure
- It provides product owners confidence on the results observed

### Counters

When running performance tests, there are two types of "counters" that provide statistics on your website or application performance.

In performance testing counters "count" the number of times specific activities occur ranging from successful page loads to errors generated. These counters provide the high-level results of a test, while individual scenarios and recordings provide the actual reasons on why they are occurring.

There are two types of counters:
- **Client-Side Counters**: Performance counters on the computer that a load test is executing, and that simulates the end user experience
- **Server-Side Counters**: Performance counters that run on servers that deliver content end users

Both counters are important when understanding the overall performance of your application.

**Client-side Counters:**

The following are some client counters that report results after performance tests.

**For each transaction:**
- Failure rate for individual transactions
- Average transaction response time (for each transaction)
- Minimum transaction response time
- Maximum transaction response time
- 90/95/98 percentile response time

**Client side activities:**
- transactions / sec
- hits / sec
- errors / sec
- average throughput

**Server-side Counters:**

The following are some server-side counters that provide additional information about your server performance.

**Processor:**
- percent processor time
- queue length
- disk queue
- thread count
- percent disk time

**Memory:**
- page faults per sec
- bytes available
- average bytes in use

**Network:**
- average requests per seconds
- average errors found per sec
- requests per sec
- bytes per sec
- memory allocated (kb)

### Top Server Monitoring Tools

While performance testing is important, it only provides a snapshot of your website or application performance under testing conditions. While this provides valuable information for fixing major issues and bottlenecks, it's also good practice to place ongoing monitoring tools on your site. These tools will alert you to new issues or environmental factors you may not have put in place during your performance tests.

Here are some of the most commonly used server monitoring tools for performance testing.

### Top Network Monitoring Tools

Along with your server, you may also want to monitor the health and activity of your network to understand potential bottlenecks and environmental issues.

Some of the most commonly used network monitoring tools for performance testing include:
- **Nagios**
- **EasyNetMonitor**

---

## KPIs Purpose

In performance testing, Key Performance Indicators (KPIs) play a pivotal role in assessing and ensuring the optimal functioning of applications under various conditions. This topic explains what KPIs are, their significance in performance testing, and how they contribute to delivering a seamless user experience.

**Use when**: Understanding KPIs in performance testing, setting failure criteria, evaluating system performance, or analyzing key metrics.

### What are KPIs in Performance Testing?

KPIs are measurable metrics that quantify the performance and effectiveness of a system. In the context of performance testing, these indicators serve as benchmarks to evaluate the behavior of applications under different loads, ensuring they meet predefined performance criteria. KPIs provide insights into the performance, reliability, and efficiency of applications.

Some of the most common KPIs are:
- Response Time
- Throughput
- Error Rate
- Concurrent Users
- Transactions Per Second (TPS)
- Resource Utilization (CPU, Memory, Disk I/O)
- Latency

### Why are KPIs Important in Performance Testing?

KPIs help testers with the following:

- **Performance Benchmarking**: KPIs set the standard for performance by establishing benchmarks that define acceptable response times, throughput, and resource utilization. These benchmarks serve as a reference point for evaluating the system's efficiency
- **Early Detection of Issues**: Monitoring KPIs during performance testing allows early identification of potential bottlenecks, latency issues, or resource constraints. This early detection enables proactive measures to be taken before these issues impact end-users
- **User Experience Optimization**: KPIs are instrumental in assessing the user experience. Metrics such as response time, error rates, and transaction rates directly impact how end-users perceive the application's performance. By optimizing these KPIs, organizations enhance user satisfaction
- **Capacity Planning**: KPIs aid in capacity planning by helping organizations understand how the system performs under different workloads. This information is invaluable for scaling resources to accommodate increasing user loads and ensuring system stability
- **Resource Utilization Analysis**: Throughput, resource consumption, and transaction rates are critical KPIs for analyzing how efficiently system resources, such as CPU, memory, and network, are utilized. This analysis guides optimizations for better resource management
- **Objective Performance Evaluation**: KPIs provide an objective and quantifiable measure of performance. This objectivity is crucial for performance testing teams, developers, and stakeholders to align their expectations and goals based on concrete data

### BlazeMeter KPIs

In BlazeMeter, you set KPIs in the **Failure Criteria** section of the test configuration page.

| Name | Description | When to Use |
|------|-------------|-------------|
| **connectTime.avg (ms)** | Average time taken for establishing a connection in milliseconds. | Use to evaluate the efficiency of connection establishment in the system. |
| **duration.count (s)** | Total duration of the performance test in seconds. | Use to understand the overall time taken for the performance test execution. |
| **errors.count** | Total count of errors observed during the performance test. | Use to identify and analyze the number of errors impacting the system. |
| **errors.percent (%)** | Percentage of errors in relation to the total number of requests. | Use to assess the impact of errors on the overall performance of the system. |
| **errors.rate (errors/s)** | Rate of errors observed per second during the performance test. | Use to monitor the frequency of errors over time. |
| **hits.avg (hits/s)** | Average number of hits (successful requests) per second. | Use to evaluate the rate of successful interactions with the system. |
| **hits.count** | Total count of successful hits during the performance test. | Use to assess the overall success rate of interactions with the system. |
| **hits.rate (hits/s)** | Rate of successful hits (interactions) per second. | Use to monitor the frequency of successful interactions over time. |
| **latency.avg (ms)** | Average latency time, measuring the time it takes for the system to respond to a request. | Use to assess the overall responsiveness of the system. |
| **responseTime.avg (ms)** | Average response time in milliseconds. | Use to assess the overall responsiveness of the application. |
| **responseTime.max (ms)** | Maximum response time observed during the performance test. | Use to identify the worst-case scenario for response time. |
| **responseTime.min (ms)** | Minimum response time observed during the performance test. | Use to identify the best-case scenario for response time. |
| **responseTime.percentile.0 (ms)** | 0th percentile response time represents the minimum time taken for a request. | Use to identify the best-case scenario for response time. |
| **responseTime.percentile.25 (ms)** | 25th percentile response time represents the time below which 25% of requests fall. | Use to understand response time distribution and identify performance outliers. |
| **responseTime.percentile.50 (ms)** | 50th percentile response time represents the median time taken for a request. | Use to understand the central tendency of response times. |
| **responseTime.percentile.90 (ms)** | 90th percentile response time represents the time below which 90% of requests fall. | Use to identify response time for a majority of requests and assess system performance. |
| **responseTime.percentile.95 (ms)** | 95th percentile response time represents the time below which 95% of requests fall. | Use to identify response time for a significant majority of requests and assess system performance. |
| **responseTime.percentile.99 (ms)** | 99th percentile response time represents the time below which 99% of requests fall. | Use to identify response time for the majority of requests and assess system performance. |
| **responseTime.std (ms)** | Standard deviation of response time, providing insights into the variability of response times. | Use to assess the consistency or variability in the system's response times. |
| **size.avg (bytes/s)** | Average size of the response payload in bytes per second. | Use to assess the efficiency of data transfer in the system. |
| **size.count** | Total count of responses during the performance test. | Use to understand the overall volume of responses generated by the system. |
| **size.rate (bytes/s)** | Rate of data transfer (response payload size) per second. | Use to monitor the efficiency of data transfer over time. |

---

## Engine Health Report

The Engine Health report displays performance indicators received from the test engines (the infrastructure delivering the test traffic, not your system under test).

**Use when**: Viewing engine health metrics, monitoring CPU and memory levels, troubleshooting high resource utilization, or determining optimal users per engine.

The Engine Health report indicates whether the test infrastructure itself could be the cause of bottlenecks or the errors which are appearing in other reports. Engine Health is also a great resource when deciding how many virtual users (VUs) each engine can support. The ideal ratio depends on the complexity and memory footprint of your script(s). You can read about the process for planning and calibration of test execution to optimally utilize available resources in the help topic, [Calibrating a BlazeMeter test](skill-blazemeter-performance-testing://references/jmeter-configuration.md).

When running performance tests, it is important to monitor the resource utilization. If one or more engines show high resource utilization, it could impact the test results.

The Engine Health report lets you view the following KPIs:
- **CPU** Represents the Percentage usage of CPU in instance
- **Memory** Represents the Percentage usage of the Virtual Memory in instance
- **Network I/O** Represents the amount of data transferred in I/O operations (KB/s)
- **Connections** Represents the number of persistent connections established for each transaction throughout the test

### View Engine Health Report

Follow these steps:

1. In the **Performance** tab, select **Reports**. The most recent reports are shown on top.
2. Click **Show all reports** and select a report to view its details. A test report overview opens.
3. Click the **Engine Health** tab. A curve diagram appears, showing the level of resource utilization over the course of the associated tests: To show/hide a KPI in the diagram, click the associated KPI label. For example, to hide Network I/O, click the **Network I/O** label in the X-axis.

### CPU and Memory Levels

When conducting performance tests, it is recommended to maintain average CPU and memory usage below 70%. The thresholds for engine instances are set at 80% for CPU and 85% for memory. These averages are calculated using a 60-second moving average of the data points collected throughout the test duration.

To illustrate, let's consider a specific 60-second period within a test run, ranging from 1 minute and 20 seconds to 2 minutes and 20 seconds. In this interval, a total of 30 measurements are taken at 2-second intervals, divided into three sets of 10 consecutive measurements each. The first set averages 50%, the second 70%, and the third 90%. As a result, the 60-second moving average for this period amounts to 70%.

If the average of the 60-second moving averages recorded throughout the entire test duration surpasses the threshold set for a particular resource type (CPU or Memory), an on-screen alert will be displayed.

During run time, these alerts include a link to the **Engine Health** tab. You can use filters in this tab to detect engines with high levels of CPU or memory, streamlining the debugging process for more effectiveness.

- To view only engines that exceeded 80% CPU usage, click **High CPU**
- To view only engines that exceeded 85% memory usage, click **High Memory**

### Troubleshooting High Levels of CPU and Memory

Ensure the following for optimal performance:
- Verify that your script is resource-efficient, avoiding enabled listeners, redundant requests, or resource-intensive samplers such as the WebDriver sampler when possible.
- Eliminate high-processing or infinite loops
- Verify all CSV files are formatted correctly
- Investigate time-outs, exceptions, and errors
- Minimize the number of users per engine.
- Consider adding more engines to enhance the test.

For more information on troubleshooting, see [3 Common Issues When Running JMeter Scripts and How to Solve Them](https://www.blazemeter.com/blog/jmeter-scripts).

### Recommendations to Reduce Number of Users per Engine

Based on the test configuration, BlazeMeter monitors resource utilization at regular intervals. If the KPIs for CPU or memory usage exceed the thresholds over the duration of the test run, a warning is triggered:

This notification indicates that you should reduce the number of virtual users per engine for future test runs.

1. In the warning, click **see recommendations**. The **Usage Recommendations** page displays:
2. Choose one of the following options: For JMX scripts, in the test row, click **Apply**. This action changes the **Max Users per Engine** value on the **Test Configuration** page according to the suggested value: For YAML scripts, edit the maximum number of users according to the suggested value on the **Users per Engine Recommendations** page: In YAML, the concept of a *max users per engine* value does not exist. Instead, a recommended value is provided on the **Users per Engine Recommendations** page. You need to ensure that none of the engines configured in the script exceed the recommended number of Virtual Users (VUs). Although this process is not as straightforward as with non-YAML scripts, it is typically an easy task. In the test row, click **Edit YAML**. The **Test Configuration** page opens. In the YAML script, check if a `locations-weighted` parameter is present and if not, add it. Ensure that `locations-weighted` is set to `false`:

```yaml
locations-weighted: false
```

Calculate the maximum users per engine using the following formula according to the suggested value on the **Users per Engine Recommendations** page:

**Engines Required = Concurrency / Recommended Maximum Users per Engine**

For example, if your test has a concurrency of 100 users and the recommendation is a maximum of 30 users per engine:

**Engines Required = 100 / 30 = 3.33 engines**

Round up the result to the nearest whole number which in this example is 4. This rounded-up value represents the total number of engines required for the specific scenario. Within your YAML script, divide the four engines among all required locations for the scenario. For example:

```yaml
concurrency: 100
locations:
  us-east4-a: 1
  eu-west-1: 3
locations-weighted: false
```

Save your YAML script. By following these steps, you can effectively modify your Taurus YAML test script when locations-weighted is set to false, ensuring that the maximum users per engine align with the recommended values while maintaining proper distribution across all locations.

---

## Baseline Comparison

Defining a test run as a baseline helps testers make sure that the application performance remains stable as the code changes. Once a baseline is defined, subsequent test runs are compared to the baseline.

**Use when**: Setting a baseline for performance comparison, identifying performance degradations, defining failure criteria using baselines, or tracking performance trends over time.

With baselines, you can easily identify degradations and bottlenecks, find the related changes in code, and quickly take actions to resolve issues.

You can select a test run as a baseline against which subsequent runs are being compared. In addition, subsequent test runs can automatically be marked as failed if the performance degrades compared to the baseline. BlazeMeter helps you automate the decision-making process, by allowing you to configure the test failure criteria using the baseline, so that test runs that significantly deviate from the baseline are automatically marked as "Failed".

### Example Scenario

Create a test and run it. Set the generated test report as a baseline. Based on the report, you can make changes to improve the system performance as needed. Run the test again and compare the new report to the previous report. If the results from the new report are better, set the new report as a baseline and also set the failure criteria against the baseline. Run the test again. If performance degrades compared to the baseline, the test will fail automatically.

### Baselines and Multi-tests

You can select a specific test run within a multi-test scenario and define it as the baseline for future comparisons.

All subsequent test runs within the multi-test scenario will automatically be compared to the defined baseline. The comparison data will be accessible in the Request Stats tab of the test report.

### Set a Report as a Baseline

Follow these steps:

1. Go to the **Performance** tab, click **Tests** and select a test from the drop down list. The test window opens.
2. Click the **History** tab.
3. Choose the report that you want to set as a baseline and click **Set as Baseline**. If you need more info on the reports to make a decision, expand the report row to see the report summary.

The report is set as a baseline. Subsequent test runs will be compared to this report.

### Remove a Baseline

1. Go to the **Performance** tab, click **Tests** and select a test from the drop down list. The test window opens.
2. Click the **History** tab.
3. To remove the baseline, just click the 'x' next to the **Baseline** button. If the baseline was used in Failure Criteria, they will need to be redefined. Or, when you click **Set as Baseline** for a different Report from your list, your current baseline will be removed and replaced with the new one you just selected.

### Set a Baseline from Report

Alternatively, you can set a baseline from the **Report** itself.

Follow these steps:

1. Go to the **Performance** tab, click **Reports** and select a report from the drop-down list. A window with report details opens.
2. In the top right corner below the report name, click **Set Report as a Baseline**.

The report is set as a baseline. Subsequent test runs will be compared to this report.

To remove the baseline, just click **Baseline** and it will revert back to **Set Report as a Baseline**. Subsequent test runs will not be compared to this report.

If the baseline was used in Failure Criteria, they will need to be redefined.

### Review Test Runs Compared Against the Baseline

Follow these steps:

1. Go to the **Performance** tab, click **Tests,** and select a test from the drop-down list. The test window opens.
2. Click the **History** tab.
3. Click one of the test reports. Make sure the test had a baseline defined when the selected report was executed. The page with the report details opens. Review the report compared to the baseline.
4. Click **Summary**. The KPIs show the change in comparison to the baseline.
5. Click **Request Stats**. To compare metrics to the baseline report, select an option from the **Baseline Comparison** dropdown. Select **Change from Baseline** to view the change in percentages from the baseline.Select **Actual Baseline values** to view the change in actual value terms.
6. Click **Failure Criteria**. You will see this tab only if failure criteria were defined against the baseline. If the failure criteria were defined using the baseline, you will see that the thresholds originated from the baseline.
7. Click **Original Test Configuration**. You can see the details of the baseline used for the current report. The baseline for the current report is the baseline that was defined in the test at the time this report was executed. If another baseline was selected after the execution of the current report - it will not affect the baseline of the report.

the baseline comparison data will be calculated and displayed only once the test run ended.

### Identify Changes in Application Performance

Follow these steps:

1. Go to the **Performance** tab, click **Tests,** and select a test from the drop-down list. The test window opens. Make sure the test has a baseline defined.
2. Click the **Trend Charts** tab.

You can see the trend charts graphs that are compared against the baseline.

### Define Failure Criteria Using the Baseline

When you set a report as a baseline, you can also set the failure criteria against the baseline. If performance degrades compared to the baseline in a subsequent test, the test will fail automatically.

Follow these steps:

1. Go to the **Performance** tab, click **Tests,** and select a test from the drop-down list. The test reports window opens.
2. Click the **Configuration** tab and scroll down to the **Failure Criteria** section.
3. Toggle the button for **Failure Criteria**on.
4. Check the box for **Use from baseline**.
5. You can: Set the threshold for each failure criteria from the selected baseline.Define failure criteria as a percentage of the baseline result. If you use Baseline to define the threshold, the threshold will be visible after saving.

Subsequent tests are compared against the baseline with these failure criteria. If performance degrades compared to the baseline, the test will fail automatically.

For more information about how to fail tests against the baseline when running from Taurus and for baseline-based Taurus failure criteria in BlazeMeter UI, see [Failure Criteria](skill-blazemeter-performance-testing://references/advanced-features.md).

For customers on accounts with **unlimited data retention**, baselines will not be archived. [View plans](https://www.blazemeter.com/pricing/#pricing-details) or contact your account manager for more information.

---

## Anomaly Detection

BlazeMeter supports anomaly detection techniques that can help teams maintain high-quality standards, ensuring that web applications perform reliably and efficiently.

**Use when**: Identifying performance anomalies in test results, detecting abnormal behavior patterns, or troubleshooting performance issues that deviate from expected behavior.

### Purpose

An anomaly, in the context of data analysis and system monitoring, is a data point or pattern that significantly deviates from the expected behavior or average trend.

Anomaly detection is part of the root cause analysis process. It surfaces issues that might not be identified by the other layers of defense, such as baseline, comparison, failure criteria, and so on. Anomaly detection involves identifying data points, events, or patterns that deviate significantly from the expected behavior. These anomalies can signal various issues such as performance degradation, security breaches, or system errors, allowing teams to address problems before they escalate.

BlazeMeter uses statistical models to implement anomaly detection.

### Benefits

- Detecting anomalies early in the testing process helps in resolving potential issues before they impact the user experience, ensuring smooth and reliable application performance
- Continuous monitoring for anomalies helps maintain optimal performance by identifying and addressing bottlenecks and other performance-related issues promptly

BlazeMeter integrates anomaly detection into its continuous testing platform in the [Timeline Report](skill-blazemeter-performance-testing://references/reporting.md).

### How BlazeMeter Detects Anomalies

BlazeMeter uses a statistical model that analyzes the response time metrics (such as average, median, 90th percentile, and so on) for every label in the test, including the All label. The model identifies abnormal behavior, defined as a significant increase or decrease in response time that deviates from the mean and exceeds a predefined threshold.

The model analyzes data points collected every second from all load generators in the test. For each data point, BlazeMeter calculates its distance from the mean, taking into account the distribution of data points by using the standard deviation. If the distance is greater than a certain threshold (X standard deviations from the mean, where X is defined by BlazeMeter), the data point is marked as abnormal.

However, a single abnormal data point does not constitute an anomaly, as occasional spikes are expected in some cases. Instead, BlazeMeter examines groups of data points over a specified time window. For a time window to be classified as an anomaly, a certain percentage of data points within that window must be marked as abnormal.

When a time window (group of data points) is identified as an anomaly, it is highlighted in **Timeline** charts with a bold purple line on the relevant label.

### Review Detected Anomalies

An **Anomalies Detected** notification in a report header indicates that the test run contains at least one anomaly.

To review each detected anomaly for further insights, follow these steps:

1. Navigate to the **Timeline** tab. Anomalies are displayed on this tab. On the left-hand KPI Selection panel, anomalies are grouped under a purple Anomalies - Response Time category. This group includes all labels and response time metrics for which anomalies were detected
2. Choose a label from the "Anomalies - Response Time" group to display it on the chart
3. Zoom in. Once the label is displayed on the chart, you may find multiple anomalies detected for that label. To focus on a specific anomaly, zoom in on the time range where the anomaly was detected
4. Adjust the chart resolution. Change the chart resolution to 1 second to view the individual data points that make up the anomaly. This detailed view allows you to observe the deviation and assess whether it indicates an issue that requires further investigation

---

## Filtering by Location and Scenario

There are two available filters in each report that allow you to focus on specific subsets of test results:

- **Location filter**: Filters the information on the report, so only users and requests that were made from a specific location will be listed
- **Scenario filter**: Filters the information on the report, so only users and requests that were generated by the requested scenario will be listed

**Use when**: Filtering performance test reports by location or scenario, focusing analysis on specific geographic locations or test scenarios, or isolating results from particular test configurations.

### Location Filter

The **Location filter** allows you to filter report data to show only results from specific geographic locations where your test was executed. This is useful when:

- Analyzing performance differences across regions
- Troubleshooting location-specific issues
- Comparing results from different geographic locations
- Focusing on results from a specific data center or region

**How to use:**

1. In a report, look for the **Location** selector in the top right corner of the report screen
2. Click the **Location** dropdown
3. Select one or more locations to filter by
4. The report will update to show only data from the selected locations

**Note**: The Location filter is disabled when there is only one location in the test run. The single location is selected and displayed by default.

### Scenario Filter

The **Scenario filter** allows you to filter report data to show only results from specific test scenarios. This is useful when:

- Analyzing individual scenarios in multi-scenario tests
- Comparing performance across different user journeys
- Isolating results from specific test configurations
- Focusing on particular business workflows

**How to use:**

1. In a report, look for the **Scenario** selector in the top right corner of the report screen
2. Click the **Scenario** dropdown
3. Select one or more scenarios to filter by
4. The report will update to show only data from the selected scenarios

**Note**: The Scenario filter is disabled when there is only one scenario in the test run. The single scenario is selected and displayed by default.

### Combining Filters

You can combine the Location and Scenario filters with other filters (such as DateTime and Transactions) to create highly focused views of your test results. This allows you to:

- Analyze specific scenarios in specific locations
- Compare performance across different combinations of locations and scenarios
- Isolate issues to particular test configurations and geographic regions

### Documentation References

For detailed information about filtering by location and scenario, use the BlazeMeter MCP help tools:

**Filtering by Location and Scenario**:
- **Category**: `root_category`
- **Subcategory**: `guide`
- **Help ID**: `performance-filter-by-location-scenario`
- **Read help**: Use `blazemeter_help` with action `read_help_info`, args: `{"category_id": "root_category", "subcategory_id": "guide", "help_id_list": ["performance-filter-by-location-scenario"]}`

---

## Original Test Configuration Report

The Original Test Configuration tab shows details about how your test was configured at the time the test covered by the report ran. This tab is especially useful if you have updated the test configuration since the report was run, as the details here will reflect the configuration prior to those updates.

**Use when**: Viewing historical test configuration details, understanding how a test was configured when a specific report was generated, or comparing current test configuration with past configurations.

### Overview

The first section you will see provides a summary of how the test was configured to run in BlazeMeter. This section provides details on how the test was configured to run (as opposed to the other report tabs, which detail how the test actually ran). It provides the following details:

- **Scenarios**: How many test scenarios were configured for the test. For example:
  - If the test was a Taurus test (executed via a YAML file), this counts how many scenarios are specified under the **scenarios:** section of the script
  - For a JMeter test (executed with a JMX, without a YAML), there will be one scenario
  - For a multi-test, there will be one scenario per test
  - An End User Experience Monitoring test will appear as its own scenario

- **Duration H:M:S**: The duration originally set for the test. Expressed in an hour:minute:second format (for example, 01:02:30 would read one hour, two minutes, and thirty seconds)

- **Total Users**: How many total users were originally configured for the entire test (all scenarios combined)

- **Locations**: How many locations were selected for the entire test (all scenarios combined) and the name of each location chosen

### Scenario Details

Next, each scenario will be detailed as it was originally configured. In this section, the following details are provided:

- **Scenario Name**: The name of the scenario. If the test executed was a Taurus test, scenario names will be pulled from the YAML file. For other types of tests, a default name will be given to each scenario, in the format of "default-scenario-xxxxxxx", where the x's are a unique number generated by BlazeMeter

- **Duration**: The original duration set for the test

- **Ramp up Duration**: How long the ramp up duration was set for. If no ramp up was set, this will not appear

- **Ramp up Steps**: How many steps the ramp up was configured for. If no steps were set, this will not appear

- **Locations**: This shows the name(s) of the location(s) assigned to the specific scenario

### End User Experience Monitoring Tests

A note about End User Experience Monitoring tests:

- **Duration** will appear as **0 Min** because the duration is actually infinite, until the end of the main test
- **Locations** will appear empty unless a location was specified in a Selenium Scenario YAML file

### Expanded Scenario Details

Click the ">" arrow on the left side of the scenario to expand it, to show additional details:

The expanded scenario details show the following information:

- **Running Info**: The **Run By** and **Created by** areas detail which user (identified by User ID and email address) ran the test and which user created the test, respectively. The **Test** area details the Test ID and the name of the individual test

- **Locations**: This expands on the previous location information, adding how many virtual users (VUs) were assigned to each location

- **Files**: All files uploaded to the test at the time the test was executed

- **Advanced**: This area captures details about any advanced settings that may have been originally configured for the test

### Historical File Versions

A special note about the scenario **Files** area: You can click each file name to download a copy of that file as it appeared at the time the test ran. These are historical versions of the files. If the files have been updated or removed since the test was run, the files here will reflect how they were *prior* to those changes being made.

### Use Cases

The Original Test Configuration Report is particularly useful for:

- **Audit and Compliance**: Understanding exactly how a test was configured for historical reports
- **Troubleshooting**: Comparing current test configuration with past configurations to identify changes
- **Reproducibility**: Recreating test conditions from historical reports
- **Documentation**: Maintaining records of test configurations over time

### Documentation References

For detailed information about the Original Test Configuration Report, use the BlazeMeter MCP help tools:

**Original Test Configuration Report**:
- **Category**: `root_category`
- **Subcategory**: `guide`
- **Help ID**: `performance-original-test-configuration-report`
- **Read help**: Use `blazemeter_help` with action `read_help_info`, args: `{"category_id": "root_category", "subcategory_id": "guide", "help_id_list": ["performance-original-test-configuration-report"]}`

---

## Documentation References

For detailed information about performance test reporting, use the BlazeMeter MCP help tools:

**Reporting**:
- **Category**: `root_category`
- **Subcategory**: `guide`
- **Help ID**: 
  - `performance-report-statuses` (report statuses)
  - `performance-exclude-failed-transactions` (exclude failed transactions)
  - `performance-filter-report-data` (filter)
  - `performance-compare-reports` (compare)
  - `performance-manage-tags-for-tests` (tags)
  - `performance-restore-archived-report` (restore)
  - `performance-share-reports` (share)
  - `performance-advanced-search` (advanced search)
  - `performance-intro-to-reporting` (intro)
  - `performance-monitoring-post-test-analysis` (monitoring)
  - `performance-kpis-purpose` (KPIs)
  - `performance-kpis-key-perf-test-metrics.htm` (key performance metrics)
  - `performance-kpis-metric-types.htm` (metric types)
  - `performance-kpis-higher-number-concurrent-users.htm` (higher concurrent users)
  - `performance-kpis-hits-in-blazemeter-reports.htm` (HITS KPI)
  - `performance-metrics-for-websites.htm` (website metrics)
  - `performance-jmeter-aggregate-report.htm` (JMeter aggregate report discrepancies)
  - `performance-anomaly-testing.htm` (anomaly detection)
  - `performance-summary-report` (summary)
  - `performance-timeline-report` (timeline)
  - `performance-errors-report` (errors)
  - `performance-request-stats-report` (request stats)
  - `performance-logs-report` (logs)
  - `performance-original-test-configuration-report` (original configuration)
  - `performance-filter-by-location-scenario` (location/scenario filter)
  - `performance-engine-health-report` (engine health)
  - `performance-baseline-comparison` (baseline comparison)
  - `performance-ignore-labels-in-reports.htm` (ignore labels)
- **Read help**: Use `blazemeter_help` with action `read_help_info`, args: `{"category_id": "root_category", "subcategory_id": "guide", "help_id_list": ["performance-report-statuses", "performance-exclude-failed-transactions", "performance-filter-report-data", "performance-compare-reports", "performance-manage-tags-for-tests", "performance-restore-archived-report", "performance-share-reports", "performance-advanced-search", "performance-intro-to-reporting", "performance-monitoring-post-test-analysis", "performance-kpis-purpose", "performance-kpis-key-perf-test-metrics.htm", "performance-kpis-metric-types.htm", "performance-kpis-higher-number-concurrent-users.htm", "performance-kpis-hits-in-blazemeter-reports.htm", "performance-metrics-for-websites.htm", "performance-jmeter-aggregate-report.htm", "performance-anomaly-testing.htm", "performance-summary-report", "performance-timeline-report", "performance-errors-report", "performance-request-stats-report", "performance-logs-report", "performance-original-test-configuration-report", "performance-filter-by-location-scenario", "performance-engine-health-report", "performance-baseline-comparison", "performance-ignore-labels-in-reports.htm"]}`

